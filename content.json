{"pages":[],"posts":[{"title":"My Start","text":"博客的写作与上传创建新文档默认创建路径 {root}/source/_posts/Name.md，创建好文档后可以使用各种编辑器进行写作 1hexo -n &quot;Article Name&quot; github端部署github上新开repo并复制下repo的地址，如下例所示： 1https://github.com/LexcaliburR/LexcaliburR.github.io.git 配置好本地的git用户 12git config --global user.email Lexcaliburr@xxxx.com # 输入自己git账户的邮箱git config --global user.name Lexcalibur # 输入git账户的用户名 安装部署用插件 1sudo cnpm install --save hexo-deployer-git 修改hexo config配置文件, 打开项目目录的_config.yml文件，在最下面添加部署方式信息，部署的github仓库地址，以及对应的分支。 123456# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: 'git' repo: https://github.com/LexcaliburR/LexcaliburR.github.io.git branch: master hexo 生成并部署博客 123hexo cleanhexo ghexo d 更换主题 1以yilia以主题为例 下载主题 1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 配置博客项目的config，打开进行部署时的_config.yml文件，找到theme（主题）部分,进行如下修改 1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: yilia # 将默认的landscape改为刚刚下载的主题 重新生成与部署，可以看到网页应用了新的主题 123hexo cleanhexo ghexo d 更换主题 2以icarus主题为例 下载主题 1git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 配置主题 1hexo config theme icarus # 或者按照上例在_config.yml中修改也可以 配置git仓库 123456# Deployment## Docs: https://hexo.io/docs/one-command-deploymentdeploy: type: 'git' repo: https://github.com/LexcaliburR/LexcaliburR.github.io.git branch: master 生成部署 123hexo cleanhexo ghexo d 修改为Cyberpunk 风格,打开{root}/_config.icarus.yml进行如下修改 12345678- variant: default+ variant: cyberpunk# Article related configurationsarticle: highlight:- theme: atom-one-light+ theme: xt256 重新生成网页，可以看到新的页面","link":"/2021/06/11/My-Start/"},{"title":"cpp 共享指针初始化","text":"1234567891011121314151617181920212223242526// Example program#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;memory&gt;struct A { int element1; int element2 = 0; std::shared_ptr&lt;int&gt; element3; std::shared_ptr&lt;int&gt; element4 = std::make_shared&lt;int&gt;(1);}; int main(){ A a; std::cout &lt;&lt; &quot;element2: &quot; &lt;&lt; a.element2 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;element1: &quot; &lt;&lt; a.element1 &lt;&lt; std::endl; std::cout &lt;&lt; &quot;element4: &quot; &lt;&lt; *a.element4 &lt;&lt; std::endl; if (a.element3) { std::cout &lt;&lt; &quot;element3: &quot; &lt;&lt; *a.element3 &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;element3 is False/Null&quot; &lt;&lt; std::endl; }} 1234element2: 0element1: 1element4: 1element3 is False/Null","link":"/2021/06/18/cpp_%E6%8C%87%E9%92%88_%E5%88%9D%E5%A7%8B%E5%8C%96%E9%97%AE%E9%A2%98/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2021/06/11/hello-world/"},{"title":"ResNet 论文阅读笔记","text":"贡献本文最大的贡献就是通过引入残差学习框架，极大的加深了可训练网络的深度。 引入当前网络发展现状到目前为止，深度卷积网络已经在图像分类领域做出了一系列重大的突破。一方面深层的网络可以整合低级/中级/高级特征。另一方面对于端到端的多层分类器而言，可以通过控制堆叠层的数量（即网络深度）控制提取出的特征的抽象级别。 近来的研究表明，网络的深度对于模型性能是非常重要的，即更深的网络意味着更大的解空间，也意味着更好的性能。 产生的问题学习更好的网络和堆叠更多的网络层一样容易吗？（Is learning better networks as easy as stacking more layers?）网络学习遇到的主要阻碍有梯度消失与梯度爆炸(vanishing/exploding gradients)，但这两个问题已经可以通过正规化初始化(normalized initialization)与中间的正规化层（intermediate normalization layers, 如BN/GBN/GN等等）极大程度上的解决了。 网络学习过程遇到的另一个问题就是网络退化问题（degradation problem），也是本文所解决的问题。 什么是网络退化随着网络深度的增加，准确度很容易饱和，之后会迅速的下降。但这个退化问题不是由过拟合导致的，当给一个合适深度的模型增加网络层时会导致更大的训练误差[1][2]。图1为典型示例。左图为20层plain network和56层plain network的训练误差，右图为对应的测试误差。 解决网络退化问题一种解决方法是通过构建模型实现：新增的网络层为恒等映射，其他的层则从对应的浅层网络拷贝。通过这种方式构建的网络被证明不会生成比对应浅层网络更高的训练误差，但是实验也表明了目前的拟合方法找到不到更优化的解。 在这篇文章中，通过引入深度残差学习框架解决了网络退化方法。对于只堆叠几层的模块而言，目前为止的办法都是直接去拟合期望的映射。我们把堆叠的这几层看作一个block，记该block的潜在映射为H(x)，我们让堆叠的网络层拟合另外一个映射F(x) = H(x) - x. 那么整个block的映射就被转换为了F（x） + x。我们假设到优化一个残差映射要比拟合原本的，无任何参考的映射要更容易，最终的结果也表明事实是这个样子的。 现在的问题在于怎么在前向传播时实现F（x） + x这个映射。 本文是通过了”short connections“实现这种变换的，如图2所示。可以通过shortcut connections跳过一层或多层。在本论文中，使用了简单的使用了恒等映射，将前一个block的输出和当前block的输出直接相加。 通过在ImageNet上与普通的“plain”网络对比，可以得到两个结论： 深度网络比“plain net”要容易优化的多，当网络层加深时，“plain net”的训练误差会增加 残差网络更容易获得准确度随着网络深度增加的的加成。 深度残差学习残差学习残差学习在上文中已进行简单的介绍，总的来说，核心在于： 退化问题表明在使用目前的优化器时，很难通过多个非线性层拟合恒等映射。 让block的参数层拟合H（x）-x 要比拟合无参考的映射简单的多。 如果最优函数更接近恒等映射而不是零映射时，优化器更容易找到相对于恒等映射的扰动而不是找到一个全新的函数。If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one. 恒等映射当层数较少时本文使用基本block，定义如下：$$ y=\\mathcal F(x,{W_i}) + x $$其中，x、y为block的输入和输出，$\\mathcal F(x,{W_i})$为需要学习的残差映射。此时x，y维度相同可直接相加。 当层数较多时使用BlttleNeck，定义如下：$$ y=\\mathcal F(x,{W_i}) + W_sx $$在使用BottleNeck时x、y通道数不一致，使用$W_s$方阵匹配输入与输出通道数。 网络架构网络架构图见table1 总结 解决退化问题，使深层卷积网络的训练成为可能。 基本不增加额外的计算量。 文献【1】 K. He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, 2015. 【2】 R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks. arXiv:1505.00387, 2015. 结束语写本文的目的主要有两个： 一个是记录整理自己学到的知识 二是因为自己对论文的理解难免有错误，所以希望能与大家进行讨论，纠正错误，共同提高 转载请著名出处，谢谢。","link":"/2021/06/18/resnet_%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"cpp","slug":"cpp","link":"/tags/cpp/"},{"name":"论文","slug":"论文","link":"/tags/%E8%AE%BA%E6%96%87/"}],"categories":[]}